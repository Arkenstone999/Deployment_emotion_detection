# streamlit_app.py

import streamlit as st
import torch
from torchvision import transforms
import cv2
from pathlib import Path

# Data handling & visualization
import pandas as pd
import numpy as np
import altair as alt
import tempfile


ENGAGEMENT_TYPES = ["not engaged", "engaged-positive", "engaged-negative"]


@st.cache_resource
def load_model() -> torch.nn.Module:
    model_path = Path("models/alexnet_full_model.pth")
    if not model_path.exists():
        st.error(
            f"âŒ Engagement model not found at:\n  {model_path}\n"
            "Please place 'alexnet_full_model.pth' in the models/ folder and rerun."
        )
        st.stop()
    model = torch.load(model_path, map_location=torch.device("cpu"))
    model.eval()
    return model


# Transformation pipeline for AlexNet (224Ã—224, normalized)
engagement_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225]),
])


@st.experimental_singleton
def load_keras_emotion_model():
    """
    Load the Keras .h5 file from 'models/best_model.h5' using standalone Keras.
    Supplies a custom DepthwiseConv2D that ignores 'groups' in its config.
    """
    model_path = BASE_DIR / "models" / "best_model.h5"
    if not model_path.exists():
        st.error(
            f"âŒ Emotion model not found at:\n  {model_path}\n"
            "Please place 'best_model.h5' in the models/ folder and rerun."
        )
        st.stop()

    # Pass custom_objects so that any DepthwiseConv2D(config={"groups":1, ...})
    # falls back to CustomDepthwiseConv2D, which pops "groups" before calling super().
    model = keras_load_model(
        str(model_path),
        custom_objects={"DepthwiseConv2D": CustomDepthwiseConv2D}
    )
    return model


@st.experimental_singleton
def get_face_cascade() -> cv2.CascadeClassifier:
    """
    Return an OpenCV Haar Cascade classifier for face detection.
    Cached so we donâ€™t reload every time.
    """
    cascade_path = cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
    face_cascade = cv2.CascadeClassifier(cascade_path)
    if face_cascade.empty():
        st.error("âŒ Failed to load Haar Cascade for face detection.")
        st.stop()
    return face_cascade


def preprocess_face_for_emotion(roi_bgr: np.ndarray, target_size=(96, 96)) -> np.ndarray:
    """
    Convert a BGR face crop (OpenCV) â†’ RGB, resize to target_size, normalize [0â€“1],
    and add a batch dimension â†’ shape (1, H, W, 3).
    """
    roi_rgb = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2RGB)
    face_resized = cv2.resize(roi_rgb, target_size, interpolation=cv2.INTER_CUBIC)
    face_norm = face_resized.astype("float32") / 255.0
    return np.expand_dims(face_norm, axis=0)


# â”€â”€ PROCESSING FUNCTIONS: ENGAGEMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_video_engagement(path: str) -> pd.DataFrame:
    """
    Run frameâ€byâ€frame inference on a video file using the engagement model.
    Returns DataFrame: ['time_sec', 'engagement_idx', 'engagement_lbl'].
    """
    model = load_engagement_model()
    cap = cv2.VideoCapture(path)
    if not cap.isOpened():
        st.error("âŒ Could not open the video file for engagement processing.")
        return pd.DataFrame()

    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    times, idxs, lbls = [], [], []
    progress_bar = st.progress(0.0)
    frame_idx = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Convert BGR â†’ RGB, transform for AlexNet
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        tensor = engagement_transform(img_rgb).unsqueeze(0)

        # Inference
        with torch.no_grad():
            outputs = model(tensor)
            _, pred = torch.max(outputs, 1)
            engagement_label = ENGAGEMENT_TYPES[pred.item()]
            engagement_index = ENGAGEMENT_INDEX_MAP[engagement_label]

        # Record timestamp / label
        times.append(frame_idx / fps)
        idxs.append(engagement_index)
        lbls.append(engagement_label)

        frame_idx += 1
        if total_frames > 0:
            progress_bar.progress(min(frame_idx / total_frames, 1.0))

    cap.release()
    progress_bar.empty()

    return pd.DataFrame({
        "time_sec": times,
        "engagement_idx": idxs,
        "engagement_lbl": lbls
    })


def process_camera_engagement(num_frames: int = 200, fps_delay: float = 0.03) -> pd.DataFrame:
    """
    Run realâ€time webcam inference for `num_frames` using the engagement model.
    Displays:
      â€¢ Live camera feed  
      â€¢ Rolling â€œ% engagedâ€ line chart  
      â€¢ Running counts text  
    Returns DataFrame: ['time_sec', 'engagement_idx', 'engagement_lbl'].
    """
    model = load_engagement_model()
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        st.error("âŒ Could not open webcam for engagement processing.")
        return pd.DataFrame()

    # Placeholders for live feed, chart, and status
    image_placeholder = st.empty()
    chart_placeholder = st.empty()
    status_text = st.empty()

    times, idxs, lbls = [], [], []
    counts = {e: 0 for e in ENGAGEMENT_TYPES}
    start_time = time.time()
    frame_idx = 0

    with st.spinner("Starting engagement webcam inference..."):
        while frame_idx < num_frames:
            ret, frame = cap.read()
            if not ret:
                status_text.error("âš ï¸ Lost camera frame during engagement run.")
                break

            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            tensor = engagement_transform(img_rgb).unsqueeze(0)

            # Inference
            with torch.no_grad():
                outputs = model(tensor)
                _, pred = torch.max(outputs, 1)
                engagement_label = ENGAGEMENT_TYPES[pred.item()]
                engagement_index = ENGAGEMENT_INDEX_MAP[engagement_label]
                counts[engagement_label] += 1

            elapsed = time.time() - start_time
            times.append(elapsed)
            idxs.append(engagement_index)
            lbls.append(engagement_label)

            # Display live frame
            image_placeholder.image(
                img_rgb,
                channels="RGB",
                caption=f"Webcam Frame {frame_idx+1}/{num_frames}"
            )

            # Rolling % engaged chart
            df_live = pd.DataFrame({
                "time_sec": times,
                "percent_engaged": [
                    ((counts["engaged-negative"] + counts["engaged-positive"]) / max(1, (i+1))) * 100
                    for i in range(len(times))
                ]
            })
            rolling_chart = (
                alt.Chart(df_live)
                .mark_line(point=True)
                .encode(
                    x=alt.X("time_sec:Q", title="Time (s)"),
                    y=alt.Y("percent_engaged:Q", title="% Engaged So Far")
                )
                .properties(height=250)
            )
            chart_placeholder.altair_chart(rolling_chart, use_container_width=True)

            status_text.text(
                f"Counts â†’ Not engaged: {counts['not engaged']}  |  "
                f"Engagedâˆ’: {counts['engaged-negative']}  |  Engaged+: {counts['engaged-positive']}"
            )

            frame_idx += 1
            time.sleep(fps_delay)

    cap.release()
    status_text.success("âœ… Engagement webcam run complete.")

    return pd.DataFrame({
        "time_sec": times,
        "engagement_idx": idxs,
        "engagement_lbl": lbls
    })


# â”€â”€ PROCESSING FUNCTIONS: EMOTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_video_emotion(path: str) -> pd.DataFrame:
    """
    Open a video file, detect faces on each frame, classify emotion on the largest face.
    Returns DataFrame: ['time_sec', 'emotion_idx', 'emotion_lbl'].
    """
    model = load_keras_emotion_model()
    face_cascade = get_face_cascade()

    cap = cv2.VideoCapture(path)
    if not cap.isOpened():
        st.error("âŒ Could not open the video file for emotion processing.")
        return pd.DataFrame()

    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    times, idxs, lbls = [], [], []
    progress_bar = st.progress(0.0)
    frame_idx = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(
            gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30)
        )

        if len(faces) > 0:
            x, y, w, h = max(faces, key=lambda r: r[2] * r[3])
            roi_bgr = frame[y : y + h, x : x + w]
            face_input = preprocess_face_for_emotion(roi_bgr, target_size=(96, 96))
            preds = model.predict(face_input, verbose=0)
            emotion_index = int(np.argmax(preds[0]))
            emotion_label = EMOTION_LABELS[emotion_index]

            times.append(frame_idx / fps)
            idxs.append(emotion_index)
            lbls.append(emotion_label)

        frame_idx += 1
        if total_frames > 0:
            progress_bar.progress(min(frame_idx / total_frames, 1.0))

    cap.release()
    progress_bar.empty()

    return pd.DataFrame({
        "time_sec": times,
        "emotion_idx": idxs,
        "emotion_lbl": lbls
    })


def process_camera_emotion(num_frames: int = 200, fps_delay: float = 0.03) -> pd.DataFrame:
    """
    Capture `num_frames` from the webcam, detect largest face each frame, classify emotion,
    draw a bounding box + label on the frame, and display live. Returns DataFrame:
    ['time_sec', 'emotion_idx', 'emotion_lbl'].
    """
    model = load_keras_emotion_model()
    face_cascade = get_face_cascade()

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        st.error("âŒ Could not open webcam for emotion processing.")
        return pd.DataFrame()

    image_placeholder = st.empty()
    status_text = st.empty()

    times, idxs, lbls = [], [], []
    start_time = time.time()
    frame_idx = 0

    with st.spinner("Starting emotion webcam inference..."):
        while frame_idx < num_frames:
            ret, frame = cap.read()
            if not ret:
                status_text.error("âš ï¸ Lost camera frame during emotion run.")
                break

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(
                gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30)
            )

            if len(faces) > 0:
                x, y, w, h = max(faces, key=lambda r: r[2] * r[3])
                roi_bgr = frame[y : y + h, x : x + w]
                face_input = preprocess_face_for_emotion(roi_bgr, target_size=(96, 96))
                preds = model.predict(face_input, verbose=0)
                emotion_index = int(np.argmax(preds[0]))
                emotion_label = EMOTION_LABELS[emotion_index]

                # Draw bounding box + label on the frame
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
                cv2.putText(
                    frame,
                    emotion_label,
                    (x, y - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.9,
                    (255, 255, 255),
                    2,
                    cv2.LINE_AA
                )

                elapsed = time.time() - start_time
                times.append(elapsed)
                idxs.append(emotion_index)
                lbls.append(emotion_label)

            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image_placeholder.image(img_rgb, channels="RGB")
            status_text.text(f"Frame {frame_idx+1}/{num_frames} | Recognized faces: {len(times)}")

            frame_idx += 1
            time.sleep(fps_delay)

    cap.release()
    status_text.success("âœ… Emotion webcam run complete.")

    return pd.DataFrame({
        "time_sec": times,
        "emotion_idx": idxs,
        "emotion_lbl": lbls
    })


# â”€â”€ STREAMLIT UI LAYOUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="Unified Facial Model App", layout="wide")
st.title("ğŸ“¹ Facial Model Playground: Engagement vs. Emotion")

st.markdown("""
Welcome! This app lets you pick **two** different facial analysis models and run them on either:
1. An uploaded video file  
2. A live webcam feed  

**Models Available**:
- ğŸŸ¢ **Engagement Model** (`alexnet_full_model.pth`) â†’ classifies each frame as:
  - not engaged (index 1)
  - engaged-negative (index 3)
  - engaged-positive (index 5)

- ğŸ”µ **Emotion Model** (`best_model.h5`) â†’ classifies each detected face into one of 7 emotions:
  `Angry, Disgusted, Fearful, Happy, Neutral, Sad, Surprised.`

Use the sidebar to select your model and input mode, then watch the real-time charts and final summary.
""")

# Sidebar: Model choice & Input mode
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    model_choice = st.radio(
        "Select a model:",
        ("Engagement Model (AlexNet)", "Emotion Model (Keras)")
    )
    input_mode = st.radio(
        "Choose input mode:",
        ("Upload Video File", "Use Webcam Live")
    )

# â”€â”€ MAIN WORKFLOW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if input_mode == "Upload Video File":
    uploaded_file = st.file_uploader(
        "Upload a video file (MP4, MOV, AVI, MKV)", 
        type=["mp4", "mov", "avi", "mkv"]
    )
    if uploaded_file is not None:
        # Write the uploaded bytes to a temp .mp4 file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_file.read())
            tmp_path = tmp.name

        st.info("ğŸ”„ Processing uploaded videoâ€”please waitâ€¦")

        if model_choice.startswith("Engagement"):
            df = process_video_engagement(tmp_path)
            if df.empty:
                st.error("âš ï¸ Engagement processing failed or no frames found.")
            else:
                st.subheader("â±ï¸ Engagement Index Over Time (Uploaded Video)")
                line_chart = (
                    alt.Chart(df)
                    .mark_line(point=True, color="#1f77b4")
                    .encode(
                        x=alt.X("time_sec:Q", title="Time (seconds)"),
                        y=alt.Y(
                            "engagement_idx:Q",
                            title="Engagement Index (1=Not, 3=Neg, 5=Pos)",
                            scale=alt.Scale(domain=[0, 6])
                        ),
                        tooltip=["time_sec", "engagement_lbl"]
                    )
                    .properties(height=350)
                )
                st.altair_chart(line_chart, use_container_width=True)

                st.subheader("ğŸ“Š Final Engagement Counts")
                counts = df["engagement_lbl"].value_counts().reindex(ENGAGEMENT_TYPES, fill_value=0)
                df_counts = pd.DataFrame({
                    "engagement": counts.index,
                    "count": counts.values
                })
                bar_chart = (
                    alt.Chart(df_counts)
                    .mark_bar()
                    .encode(
                        x="engagement",
                        y="count",
                        color=alt.Color("engagement", legend=None),
                        tooltip=["count"]
                    )
                    .properties(height=350)
                )
                st.altair_chart(bar_chart, use_container_width=True)

        else:  # Emotion Model
            df = process_video_emotion(tmp_path)
            if df.empty:
                st.error("âš ï¸ Emotion processing failed or no faces detected.")
            else:
                st.subheader("â±ï¸ Emotion Index Over Time (Uploaded Video)")
                line_chart = (
                    alt.Chart(df)
                    .mark_line(point=True, color="#ff7f0e")
                    .encode(
                        x=alt.X("time_sec:Q", title="Time (seconds)"),
                        y=alt.Y(
                            "emotion_idx:Q",
                            title="Emotion Index (0=Angry â€¦ 6=Surprised)",
                            scale=alt.Scale(domain=[-0.5, 6.5])
                        ),
                        tooltip=["time_sec", "emotion_lbl"]
                    )
                    .properties(height=350)
                )
                st.altair_chart(line_chart, use_container_width=True)

                st.subheader("ğŸ“Š Final Emotion Counts")
                counts = df["emotion_lbl"].value_counts().reindex(EMOTION_LABELS, fill_value=0)
                df_counts = pd.DataFrame({
                    "emotion": counts.index,
                    "count": counts.values
                })
                bar_chart = (
                    alt.Chart(df_counts)
                    .mark_bar()
                    .encode(
                        x="emotion",
                        y="count",
                        color=alt.Color("emotion", legend=None),
                        tooltip=["count"]
                    )
                    .properties(height=350)
                )
                st.altair_chart(bar_chart, use_container_width=True)

elif input_mode == "Use Webcam Live":
    st.info("â–¶ï¸ Click the button below to start webcam capture.")
    if st.button("â–¶ï¸ Start Webcam"):
        if model_choice.startswith("Engagement"):
            df = process_camera_engagement(num_frames=200, fps_delay=0.03)
            if df.empty:
                st.error("âš ï¸ Engagement webcam processing failed or no frames found.")
            else:
                st.subheader("â±ï¸ Engagement Index Over Time (Webcam Run)")
                line_chart = (
                    alt.Chart(df)
                    .mark_line(point=True, color="#1f77b4")
                    .encode(
                        x=alt.X("time_sec:Q", title="Time (seconds)"),
                        y=alt.Y(
                            "engagement_idx:Q",
                            title="Engagement Index (1=Not, 3=Neg, 5=Pos)",
                            scale=alt.Scale(domain=[0, 6])
                        ),
                        tooltip=["time_sec", "engagement_lbl"]
                    )
                    .properties(height=350)
                )
                st.altair_chart(line_chart, use_container_width=True)

                st.subheader("ğŸ“Š Final Engagement Counts (Webcam Run)")
                counts = df["engagement_lbl"].value_counts().reindex(ENGAGEMENT_TYPES, fill_value=0)
                df_counts = pd.DataFrame({
                    "engagement": counts.index,
                    "count": counts.values
                })
                bar_chart = (
                    alt.Chart(df_counts)
                    .mark_bar()
                    .encode(
                        x="engagement",
                        y="count",
                        color=alt.Color("engagement", legend=None),
                        tooltip=["count"]
                    )
                    .properties(height=350)
                )
                st.altair_chart(bar_chart, use_container_width=True)

        else:  # Emotion Model
            df = process_camera_emotion(num_frames=200, fps_delay=0.03)
            if df.empty:
                st.error("âš ï¸ Emotion webcam processing failed or no faces detected.")
            else:
                st.subheader("â±ï¸ Emotion Index Over Time (Webcam Run)")
                line_chart = (
                    alt.Chart(df)
                    .mark_line(point=True, color="#ff7f0e")
                    .encode(
                        x=alt.X("time_sec:Q", title="Time (seconds)"),
                        y=alt.Y(
                            "emotion_idx:Q",
                            title="Emotion Index (0=Angry â€¦ 6=Surprised)",
                            scale=alt.Scale(domain=[-0.5, 6.5])
                        ),
                        tooltip=["time_sec", "emotion_lbl"]
                    )
                    .properties(height=350)
                )
                st.altair_chart(line_chart, use_container_width=True)

                st.subheader("ğŸ“Š Final Emotion Counts (Webcam Run)")
                counts = df["emotion_lbl"].value_counts().reindex(EMOTION_LABELS, fill_value=0)
                df_counts = pd.DataFrame({
                    "emotion": counts.index,
                    "count": counts.values
                })
                bar_chart = (
                    alt.Chart(df_counts)
                    .mark_bar()
                    .encode(
                        x="emotion",
                        y="count",
                        color=alt.Color("emotion", legend=None),
                        tooltip=["count"]
                    )
                    .properties(height=350)
                )
                st.altair_chart(bar_chart, use_container_width=True)